{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iLB_B2SeF9TD",
        "outputId": "c5063115-5db4-4e06-d16a-1b816185919e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install pygame gymnasium\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p6gYLyFNGO8K",
        "outputId": "eaa47dfa-45d0-4d66-ff56-5c96cd268539"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pygame in /usr/local/lib/python3.10/dist-packages (2.6.0)\n",
            "Collecting gymnasium\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.12.2)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Installing collected packages: farama-notifications, gymnasium\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-0.29.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "from collections import deque\n",
        "from tensorflow.keras.layers import Input, Dense, Concatenate, Conv2D, Flatten,MaxPool2D\n",
        "from tensorflow.keras.models import Model,Sequential\n",
        "from tensorflow.keras.optimizers import Adam,SGD\n",
        "from tensorflow.keras.losses import MeanSquaredError, MSE,mean_squared_error\n",
        "from tensorflow import reduce_mean, convert_to_tensor, squeeze, float32, GradientTape\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import time\n",
        "from PIL import Image\n",
        "import os"
      ],
      "metadata": {
        "id": "bMX1igEFGWBy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DDQN:\n",
        "    def __init__(self, state_space_shape, num_actions, model, target_model, learning_rate=0.1,\n",
        "                 discount_factor=0.95, batch_size=16, memory_size=100):\n",
        "        \"\"\"\n",
        "        Initializes Double Deep Q Network agent.\n",
        "        :param state_space_shape: shape of the observation space\n",
        "        :param num_actions: number of actions\n",
        "        :param model: Keras model\n",
        "        :param target_model: Keras model\n",
        "        :param learning_rate: learning rate\n",
        "        :param discount_factor: discount factor\n",
        "        :param batch_size: batch size\n",
        "        :param memory_size: maximum size of the experience replay memory\n",
        "        \"\"\"\n",
        "        self.state_space_shape = state_space_shape\n",
        "        self.num_actions = num_actions\n",
        "        self.learning_rate = learning_rate\n",
        "        self.discount_factor = discount_factor\n",
        "        self.batch_size = batch_size\n",
        "        self.memory = deque(maxlen=memory_size)\n",
        "        self.model = model\n",
        "        self.target_model = target_model\n",
        "        self.update_target_model()\n",
        "\n",
        "    def update_memory(self, state, action, reward, next_state, done):\n",
        "        \"\"\"\n",
        "        Adds experience tuple to experience replay memory.\n",
        "        :param state: current state\n",
        "        :param action: performed action\n",
        "        :param reward: reward received for performing action\n",
        "        :param next_state: next state\n",
        "        :param done: if episode has terminated after performing the action in the current state\n",
        "        \"\"\"\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def update_target_model(self):\n",
        "        \"\"\"\n",
        "        Synchronize the target model with the main model.\n",
        "        \"\"\"\n",
        "        self.target_model.set_weights(self.model.get_weights())\n",
        "\n",
        "    def get_action(self, state, epsilon):\n",
        "        \"\"\"\n",
        "        Returns the best action following epsilon greedy policy for the current state.\n",
        "        :param state: current state\n",
        "        :param epsilon: exploration rate\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        probability = np.random.random() + epsilon / self.num_actions\n",
        "        if probability < epsilon:\n",
        "            return np.random.randint(0, self.num_actions)\n",
        "        else:\n",
        "            if isinstance(self.state_space_shape, tuple):\n",
        "                state = state.reshape((1,) + self.state_space_shape)\n",
        "            else:\n",
        "                state = state.reshape(1, self.state_space_shape)\n",
        "            return np.argmax(self.model.predict(state,verbose=0)[0])\n",
        "\n",
        "    def load(self, model_name, episode):\n",
        "        path=\"/content/drive/MyDrive/Colab Notebooks/abs_206009\"\n",
        "        \"\"\"\n",
        "        Loads the weights of the model at specified episode checkpoint.\n",
        "        :param model_name: name of the model\n",
        "        :param episode: episode checkpoint\n",
        "        \"\"\"\n",
        "        self.model.load_weights(f'{path}/weights/{model_name}_{episode}.h5')\n",
        "\n",
        "    def save(self, model_name, episode):\n",
        "        \"\"\"\n",
        "        Stores the weights of the model at specified episode checkpoint.\n",
        "        :param model_name: name of the model\n",
        "        :param episode: episode checkpoint\n",
        "        \"\"\"\n",
        "        path=\"/content/drive/MyDrive/Colab Notebooks/abs_206009\"\n",
        "        if not os.path.exists(f\"{path}/weights/\"):\n",
        "            os.mkdir(f\"{path}/weights\")\n",
        "        self.model.save_weights(f'{path}/weights/{model_name}_{episode}.h5')\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"\n",
        "        Performs one step of model training.\n",
        "        \"\"\"\n",
        "        batch_size = min(self.batch_size, len(self.memory))\n",
        "        minibatch = random.sample(self.memory, batch_size)\n",
        "\n",
        "        if isinstance(self.state_space_shape, tuple):\n",
        "            states = np.zeros((batch_size,) + self.state_space_shape)\n",
        "        else:\n",
        "            states = np.zeros((batch_size, self.state_space_shape))\n",
        "        actions = np.zeros((batch_size, self.num_actions))\n",
        "\n",
        "        for i in range(len(minibatch)):\n",
        "            state, action, reward, next_state, done = minibatch[i]\n",
        "            if done:\n",
        "                max_future_q = reward\n",
        "            else:\n",
        "                if isinstance(self.state_space_shape, tuple):\n",
        "                    next_state = next_state.reshape((1,) + self.state_space_shape)\n",
        "                else:\n",
        "                    next_state = next_state.reshape(1, self.state_space_shape)\n",
        "                max_action = np.argmax(self.model.predict(next_state,verbose=0)[0])\n",
        "                max_future_q = (reward + self.discount_factor *\n",
        "                                self.target_model.predict(next_state,verbose=0)[0][max_action])\n",
        "            if isinstance(self.state_space_shape, tuple):\n",
        "                state = state.reshape((1,) + self.state_space_shape)\n",
        "            else:\n",
        "                state = state.reshape(1, self.state_space_shape)\n",
        "            target_q = self.model.predict(state,verbose=0)[0]\n",
        "            target_q[action] = max_future_q\n",
        "            states[i] = state\n",
        "            actions[i] = target_q\n",
        "\n",
        "        self.model.train_on_batch(states, actions)"
      ],
      "metadata": {
        "id": "9_O6Y6xGGX0D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model1(state_space_shape, num_actions):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(16, input_shape=state_space_shape))\n",
        "    model.add(Dense(16))\n",
        "    model.add(Dense(num_actions, activation='linear'))\n",
        "    model.compile(SGD(0.001), mean_squared_error)\n",
        "    return model"
      ],
      "metadata": {
        "id": "RUSmZjBXQm7A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model2(state_space_shape, num_actions):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(state_space_shape[0], input_shape=state_space_shape))\n",
        "    model.add(Dense(12,activation=\"relu\"))\n",
        "    model.add(Dense(16, activation=\"relu\"))\n",
        "    model.add(Dense(8, activation=\"relu\"))\n",
        "    model.add(Dense(num_actions, activation='linear'))\n",
        "    model.compile(Adam(learning_rate=0.001), mean_squared_error)\n",
        "    return model"
      ],
      "metadata": {
        "id": "_xLUvH20HUjm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model3(state_space_shape,num_actions):\n",
        "\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(state_space_shape[0], input_shape=state_space_shape,activation=\"relu\"))\n",
        "    model.add(Dense(16, activation=\"relu\"))\n",
        "    model.add(Dense(8, activation=\"relu\"))\n",
        "    model.add(Dense(num_actions, activation=\"linear\"))\n",
        "\n",
        "    model.compile(loss=\"mse\", optimizer=Adam(learning_rate=0.0001))\n",
        "    return model"
      ],
      "metadata": {
        "id": "gCxB6wlrRPbb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(\"Acrobot-v1\")\n",
        "state,_=env.reset()"
      ],
      "metadata": {
        "id": "91WRoR6jHXj1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "state_space_shape = env.observation_space.shape\n",
        "num_actions = env.action_space.n\n",
        "num_episodes = 600\n",
        "num_steps_per_episode = 1000\n",
        "epsilon = 0.35\n",
        "epsilon_decay=0.95\n",
        "epsilon_final=0.1"
      ],
      "metadata": {
        "id": "aKUGDTyoHYyM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_model1(state_space_shape, num_actions)\n",
        "target_model = build_model1(state_space_shape, num_actions)\n",
        "agent = DDQN(state_space_shape, num_actions, model, target_model,memory_size=1000000)\n",
        "\n",
        "for episode in range(1, num_episodes + 1):\n",
        "    rewards = 0\n",
        "    steps = 0\n",
        "    state, _ = env.reset()\n",
        "    terminated = False\n",
        "    while not terminated:\n",
        "        steps += 1\n",
        "        action = agent.get_action(state, epsilon)\n",
        "        new_state, reward, terminated, _, _ = env.step(action)\n",
        "\n",
        "        if terminated:\n",
        "            reward=int(10_000/steps)\n",
        "\n",
        "        rewards += reward\n",
        "\n",
        "        agent.update_memory(state, action, reward, new_state, terminated)\n",
        "        state = new_state\n",
        "\n",
        "    print(f\"Episode {episode}/{num_episodes}, reward: {rewards}\")\n",
        "\n",
        "    agent.train()\n",
        "    epsilon *= epsilon_decay\n",
        "    epsilon=max(epsilon,epsilon_final)\n",
        "\n",
        "    if episode % 10 == 0:\n",
        "        agent.save(\"ddqn_acrobot_model1\", episode)\n",
        "        agent.update_target_model()\n"
      ],
      "metadata": {
        "id": "NKp-C8KeHpLP",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_model2(state_space_shape, num_actions)\n",
        "target_model = build_model2(state_space_shape, num_actions)\n",
        "agent = DDQN(state_space_shape, num_actions, model, target_model,memory_size=1000000)\n",
        "\n",
        "for episode in range(1, num_episodes + 1):\n",
        "    rewards = 0\n",
        "    steps = 0\n",
        "    state, _ = env.reset()\n",
        "    terminated = False\n",
        "    while not terminated:\n",
        "        steps += 1\n",
        "        action = agent.get_action(state, epsilon)\n",
        "        new_state, reward, terminated, _, _ = env.step(action)\n",
        "\n",
        "        if terminated:\n",
        "            reward=int(10_000/steps)\n",
        "\n",
        "        rewards += reward\n",
        "\n",
        "        agent.update_memory(state, action, reward, new_state, terminated)\n",
        "        state = new_state\n",
        "\n",
        "    print(f\"Episode {episode}/{num_episodes}, reward: {rewards}\")\n",
        "\n",
        "    agent.train()\n",
        "    epsilon *= epsilon_decay\n",
        "    epsilon=max(epsilon,epsilon_final)\n",
        "\n",
        "    if episode % 10 == 0:\n",
        "        agent.save(\"ddqn_acrobot_model2\", episode)\n",
        "        agent.update_target_model()\n"
      ],
      "metadata": {
        "id": "CE1mLGT3BWUI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_model3(state_space_shape, num_actions)\n",
        "target_model = build_model3(state_space_shape, num_actions)\n",
        "agent = DDQN(state_space_shape, num_actions, model, target_model,memory_size=1000000)\n",
        "\n",
        "for episode in range(1, num_episodes + 1):\n",
        "    rewards = 0\n",
        "    steps = 0\n",
        "    state, _ = env.reset()\n",
        "    terminated = False\n",
        "    while not terminated:\n",
        "        steps += 1\n",
        "        action = agent.get_action(state, epsilon)\n",
        "        new_state, reward, terminated, _, _ = env.step(action)\n",
        "\n",
        "        if terminated:\n",
        "            reward=int(10_000/steps)\n",
        "\n",
        "        rewards += reward\n",
        "\n",
        "        agent.update_memory(state, action, reward, new_state, terminated)\n",
        "        state = new_state\n",
        "\n",
        "    print(f\"Episode {episode}/{num_episodes}, reward: {rewards}\")\n",
        "\n",
        "    agent.train()\n",
        "    epsilon *= epsilon_decay\n",
        "    epsilon=max(epsilon,epsilon_final)\n",
        "\n",
        "    if episode % 10 == 0:\n",
        "        agent.save(\"ddqn_acrobot_model3\", episode)\n",
        "        agent.update_target_model()\n"
      ],
      "metadata": {
        "id": "HOWZnaXzBacZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}